{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7b0471",
   "metadata": {},
   "source": [
    "# MoleculeNet - ESOL Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe8e806",
   "metadata": {},
   "source": [
    "Most of the times, you want to perform multiple runs to find the best architecture of your model. Manual logging and management of the configurations is extremely inefficient. For this purpose, you can leverage multiple existing tool:\n",
    "- PyTorch Lightning (Torch wrapper to log, remove boilerplate code, automate runs, multi-gpu training)\n",
    "- Hydra (configuration management, perform hyperparameters sweep)\n",
    "- Optuna (hyperparameters search and optimization)\n",
    "- Weight and Biases/MLFlow (logging, hyperparameters search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f645eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GINConv, global_add_pool, GCNConv, GraphConv\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim=128, num_layers=5, dropout=0.2, gnn_type=\"GIN\"):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "        # first layer\n",
    "        if gnn_type == \"GIN\":\n",
    "            nn1 = nn.Sequential(nn.Linear(in_channels, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.convs.append(GINConv(nn1))\n",
    "        elif gnn_type == \"GCN\":\n",
    "            self.convs.append(GCNConv(in_channels, hidden_dim))\n",
    "        else:\n",
    "            self.convs.append(GraphConv(in_channels, hidden_dim))\n",
    "\n",
    "\n",
    "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "\n",
    "        for _ in range(num_layers - 1):\n",
    "            if gnn_type == \"GIN\":\n",
    "                nnk = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.convs.append(GINConv(nnk))\n",
    "            elif gnn_type == \"GCN\":\n",
    "                self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "            else:\n",
    "                self.convs.append(GraphConv(hidden_dim, hidden_dim))\n",
    "                self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "            self.dropout = dropout\n",
    "            self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "\n",
    "            x = global_add_pool(x, batch)\n",
    "            out = self.mlp(x).squeeze(-1)\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f963028",
   "metadata": {},
   "source": [
    "## Lightning setup\n",
    "To run with lighning, we shuold setup a LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc48f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MoleculeNetDataModule(LightningDataModule):\n",
    "    def __init__(self, root, name, batch_size=64, num_workers=4, pin_memory=True, seed=42):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.name = name\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.pin_memory = pin_memory\n",
    "        self.seed = seed\n",
    "\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download if needed\n",
    "        MoleculeNet(self.root, name=self.name)\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = MoleculeNet(self.root, name=self.name)\n",
    "\n",
    "\n",
    "        # MoleculeNet returns a dataset with .y as targets for regression tasks\n",
    "        # We'll create a reproducible random split (80/10/10)\n",
    "        n = len(dataset)\n",
    "        idx = list(range(n))\n",
    "\n",
    "\n",
    "        train_idx, test_idx = train_test_split(idx, test_size=0.2, random_state=self.seed)\n",
    "        val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=self.seed)\n",
    "\n",
    "\n",
    "        self.train_dataset = dataset[train_idx]\n",
    "        self.val_dataset = dataset[val_idx]\n",
    "        self.test_dataset = dataset[test_idx]\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers, pin_memory=self.pin_memory)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=self.pin_memory)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers, pin_memory=self.pin_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1db89221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torchmetrics.functional import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LitGNN(pl.LightningModule):\n",
    "    def __init__(self, in_channels, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(OmegaConf.to_container(cfg, resolve=True))\n",
    "        self.model = GNN(in_channels, hidden_dim=cfg.model.hidden_dim, num_layers=cfg.model.num_layers, dropout=cfg.model.dropout, gnn_type=cfg.model.gnn_type)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.val_preds, self.val_targets = [], []\n",
    "        self.test_preds, self.test_targets = [], []\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        return self.model(x, edge_index, batch)\n",
    "\n",
    "\n",
    "    def step(self, batch, stage):\n",
    "        y = batch.y.view(-1).float()\n",
    "        pred = self(batch.x, batch.edge_index, batch.batch)\n",
    "        loss = self.criterion(pred, y)\n",
    "        return loss, pred.detach().cpu().numpy(), y.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _, _ = self.step(batch, 'train')\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, pred, y = self.step(batch, 'val')\n",
    "        self.val_preds.append(pred)\n",
    "        self.val_targets.append(y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = np.concatenate(self.val_preds)\n",
    "        ys = np.concatenate(self.val_targets)\n",
    "        mse = torch.mean((torch.tensor(preds) - torch.tensor(ys)) ** 2).item()\n",
    "        rmse = mse ** 0.5\n",
    "        self.log(\"val_rmse\", rmse, prog_bar=True)\n",
    "        self.val_preds.clear()\n",
    "        self.val_targets.clear()\n",
    "    \n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     y = batch.y.view(-1).float()\n",
    "    #     pred = self(batch.x, batch.edge_index, batch.batch)\n",
    "    #     loss = self.criterion(pred, y)\n",
    "    #     # optionally log metrics\n",
    "    #     self.log(\"test_loss\", loss)\n",
    "    #     return {\"loss\": loss, \"pred\": pred, \"y\": y}\n",
    "\n",
    "    # def on_test_epoch_end(self):\n",
    "    #     preds = torch.cat(self.test_preds).numpy()\n",
    "    #     ys = torch.cat(self.test_targets).numpy()\n",
    "    #     mse = np.mean((preds - ys) ** 2)\n",
    "    #     rmse = mse ** 0.5\n",
    "    #     mae = mean_absolute_error(ys, preds)\n",
    "    #     r2 = r2_score(ys, preds)\n",
    "    #     self.log(\"test_rmse\", rmse)\n",
    "    #     self.log(\"test_mae\", mae)\n",
    "    #     self.log(\"test_r2\", r2)\n",
    "    #     # clear memory\n",
    "    #     self.test_preds.clear()\n",
    "    #     self.test_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.hparams['optim']['optimizer'].lower() == 'adam':\n",
    "            opt = torch.optim.Adam(self.parameters(), lr=self.hparams['optim']['lr'], weight_decay=self.hparams['optim']['weight_decay'])\n",
    "        else:\n",
    "            opt = torch.optim.SGD(self.parameters(), lr=self.hparams['optim']['lr'], weight_decay=self.hparams['optim']['weight_decay'])\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3beccb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg: DictConfig):\n",
    "\n",
    "    pl.seed_everything(cfg.seed)\n",
    "\n",
    "\n",
    "    # datamodule\n",
    "    dm = MoleculeNetDataModule(root=cfg.data.root, name=cfg.data.name, batch_size=cfg.data.batch_size, num_workers=cfg.data.num_workers, pin_memory=cfg.data.pin_memory, seed=cfg.seed)\n",
    "    dm.prepare_data()\n",
    "    dm.setup()\n",
    "\n",
    "\n",
    "    # infer input channels from first sample\n",
    "    sample = dm.train_dataset[0]\n",
    "    in_channels = sample.x.shape[1]\n",
    "\n",
    "\n",
    "    # model\n",
    "    lit = LitGNN(in_channels=in_channels, cfg=cfg)\n",
    "\n",
    "\n",
    "    # logger (CSV logger will create metrics CSV files)\n",
    "    csv_logger = CSVLogger(save_dir=cfg.logging.csv_log_dir, name=cfg.logging.name)\n",
    "\n",
    "\n",
    "    # callbacks\n",
    "    checkpoint_cb = ModelCheckpoint(dirpath=os.path.join(csv_logger.log_dir, \"checkpoints\"), save_top_k=cfg.callbacks.checkpoint_top_k, monitor='val_rmse', mode='min')\n",
    "    early_stop_cb = EarlyStopping(monitor='val_rmse', patience=cfg.callbacks.early_stopping_patience, mode='min')\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=cfg.trainer.max_epochs, logger=csv_logger, callbacks=[checkpoint_cb, early_stop_cb], precision=cfg.trainer.precision)\n",
    "\n",
    "\n",
    "    trainer.fit(lit, datamodule=dm)\n",
    "    # trainer.test(lit, datamodule=dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b08724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model     | GNN     | 158 K  | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "158 K     Trainable params\n",
      "0         Non-trainable params\n",
      "158 K     Total params\n",
      "0.634     Total estimated model params size (MB)\n",
      "40        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:00<00:00, 37.06it/s, v_num=11, val_loss=3.460, val_rmse=1.860, train_loss=1.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 15/15 [00:00<00:00, 36.63it/s, v_num=11, val_loss=3.460, val_rmse=1.860, train_loss=1.100]\n"
     ]
    }
   ],
   "source": [
    "from hydra.core.global_hydra import GlobalHydra\n",
    "\n",
    "# Reset Hydra if already initialized (for repeated runs in notebooks)\n",
    "if GlobalHydra.instance().is_initialized():\n",
    "    GlobalHydra.instance().clear()\n",
    "\n",
    "from hydra import compose, initialize\n",
    "\n",
    "initialize(config_path=\"../config\", version_base=None)\n",
    "cfg = compose(config_name=\"config\")\n",
    "main(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
